{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import preprocfs as pre\n",
    "from hyperopt import Trials,STATUS_OK,tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice,uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    tf_seed = 720\n",
    "    np_seed = 1995\n",
    "    dev_proportion = 10/100\n",
    "    data_path = 'ConstrictionDegreeOnly/task2maeda_a_zero/data/'\n",
    "    # import data into dataframe and verify dataframe dimensions\n",
    "    x_train_dev = pd.read_csv(data_path + 'x_train.csv',index_col=0,header=0)\n",
    "    x_cols = x_train_dev.columns\n",
    "    y_train_dev = pd.read_csv(data_path + 'y_train.csv',index_col=0,header=0)\n",
    "    y_cols = y_train_dev.columns\n",
    "    x_train_raw = pd.read_csv(data_path + 'x_train_raw.csv',index_col=0,header=0).to_numpy()\n",
    "    y_train_raw = pd.read_csv(data_path + 'y_train_raw.csv',index_col=0,header=0).to_numpy()\n",
    "    # repeatability\n",
    "    tf.random.set_seed(tf_seed)\n",
    "    np.random.seed(np_seed)\n",
    "\n",
    "    # split data\n",
    "    [x_train,y_train,x_dev,y_dev] = pre.split_data(x_train_dev,y_train_dev,dev_proportion,numpy=True)\n",
    "    return x_train,y_train,x_dev,y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and model architecture\n",
    "def create_model():\n",
    "    epochs = 600\n",
    "    batch_size = {{choice([64,128,256])}}\n",
    "    n_hidden_layers = {{choice([3,4,5])}}\n",
    "    n_units_per_layer = {{choice([128,256,512])}}\n",
    "    act = 'relu'\n",
    "    learning_rate = {{choice([0.01,0.001,0.0001])}}\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss_function = 'mse'\n",
    "    dev_proportion = 10/100\n",
    "    tf_seed = 720\n",
    "    np_seed = 1995\n",
    "    # repeatability\n",
    "    tf.random.set_seed(tf_seed)\n",
    "    np.random.seed(np_seed)\n",
    "    # build model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input(x_train.shape[1],))\n",
    "    for n in range(n_hidden_layers):\n",
    "        model.add(tf.keras.layers.Dense(n_units_per_layer,activation=act))\n",
    "    model.add(tf.keras.layers.Dense(y_train.shape[1]))\n",
    "    model.compile(optimizer=optimizer, loss=loss_function)\n",
    "    history = model.fit(x_train,y_train,validation_data=(x_dev,y_dev),batch_size=batch_size,epochs=epochs,verbose=False)\n",
    "    loss = model.evaluate(x_dev,y_dev,batch_size=batch_size,verbose=0)\n",
    "    return {'loss': loss, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import time\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import preprocfs as pre\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'batch_size': hp.choice('batch_size', [64,128,256]),\n",
      "        'n_hidden_layers': hp.choice('n_hidden_layers', [3,4,5]),\n",
      "        'n_units_per_layer': hp.choice('n_units_per_layer', [128,256,512]),\n",
      "        'learning_rate': hp.choice('learning_rate', [0.01,0.001,0.0001]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: tf_seed = 720\n",
      "  3: np_seed = 1995\n",
      "  4: dev_proportion = 10/100\n",
      "  5: data_path = 'ConstrictionDegreeOnly/task2maeda_a_zero/data/'\n",
      "  6: # import data into dataframe and verify dataframe dimensions\n",
      "  7: x_train_dev = pd.read_csv(data_path + 'x_train.csv',index_col=0,header=0)\n",
      "  8: x_cols = x_train_dev.columns\n",
      "  9: y_train_dev = pd.read_csv(data_path + 'y_train.csv',index_col=0,header=0)\n",
      " 10: y_cols = y_train_dev.columns\n",
      " 11: x_train_raw = pd.read_csv(data_path + 'x_train_raw.csv',index_col=0,header=0).to_numpy()\n",
      " 12: y_train_raw = pd.read_csv(data_path + 'y_train_raw.csv',index_col=0,header=0).to_numpy()\n",
      " 13: # repeatability\n",
      " 14: tf.random.set_seed(tf_seed)\n",
      " 15: np.random.seed(np_seed)\n",
      " 16: \n",
      " 17: # split data\n",
      " 18: [x_train,y_train,x_dev,y_dev] = pre.split_data(x_train_dev,y_train_dev,dev_proportion,numpy=True)\n",
      " 19: \n",
      " 20: \n",
      " 21: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     epochs = 600\n",
      "   4:     batch_size = space['batch_size']\n",
      "   5:     n_hidden_layers = space['n_hidden_layers']\n",
      "   6:     n_units_per_layer = space['n_units_per_layer']\n",
      "   7:     act = 'relu'\n",
      "   8:     learning_rate = space['learning_rate']\n",
      "   9:     optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
      "  10:     loss_function = 'mse'\n",
      "  11:     dev_proportion = 10/100\n",
      "  12:     tf_seed = 720\n",
      "  13:     np_seed = 1995\n",
      "  14:     # repeatability\n",
      "  15:     tf.random.set_seed(tf_seed)\n",
      "  16:     np.random.seed(np_seed)\n",
      "  17:     # build model\n",
      "  18:     model = tf.keras.Sequential()\n",
      "  19:     model.add(tf.keras.Input(x_train.shape[1],))\n",
      "  20:     for n in range(n_hidden_layers):\n",
      "  21:         model.add(tf.keras.layers.Dense(n_units_per_layer,activation=act))\n",
      "  22:     model.add(tf.keras.layers.Dense(y_train.shape[1]))\n",
      "  23:     model.compile(optimizer=optimizer, loss=loss_function)\n",
      "  24:     history = model.fit(x_train,y_train,validation_data=(x_dev,y_dev),batch_size=batch_size,epochs=epochs,verbose=False)\n",
      "  25:     loss = model.evaluate(x_dev,y_dev,batch_size=batch_size,verbose=0)\n",
      "  26:     return {'loss': loss, 'status': STATUS_OK, 'model': model}\n",
      "  27: \n",
      "100%|█████████████████████████████████████████| 50/50 [13:14:30<00:00, 953.41s/trial, best loss: 3.667412255533801e-05]\n",
      "{'batch_size': 0, 'learning_rate': 2, 'n_hidden_layers': 2, 'n_units_per_layer': 1}\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_val,y_val = data()\n",
    "best_run, best_model = optim.minimize(\n",
    "                        model=create_model,\n",
    "                        data=data,\n",
    "                        algo=tpe.suggest,\n",
    "                        max_evals=50,\n",
    "                        trials=Trials(),\n",
    "                        notebook_name='hyperparameter-optimization'\n",
    ")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292860/292860 [==============================] - 11s 37us/sample - loss: 3.2640e-05\n",
      "3.263997600156943e-05\n"
     ]
    }
   ],
   "source": [
    "data_path = 'ConstrictionDegreeOnly/task2maeda_a_zero/data/'\n",
    "x_test = pd.read_csv(data_path + 'x_test.csv',index_col=0,header=0).to_numpy()\n",
    "y_test = pd.read_csv(data_path + 'y_test.csv',index_col=0,header=0).to_numpy()\n",
    "print(best_model.evaluate(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
